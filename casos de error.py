# -*- coding: utf-8 -*-
"""pruebaNeuroas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HVmgSkRl-CkJ8YHfWruxLtXdHKCBhVV7

https://www.youtube.com/watch?v=Tb8f_KCjCjI

## Librerias
"""

# IMPORTANDO LIBRERIAS
import numpy                       as     np                    # algebra lineal
from tensorflow.keras.models       import Sequential            # modelo secuencial
from tensorflow.keras.layers       import Dense                 # capas
from tensorflow.keras.layers       import BatchNormalization    # normailizaciòn de lotes
#from tensorflow.keras.layers       import Dropout               # dropout
from tensorflow.keras.optimizers   import SGD                   # gradiente descendente
from tensorflow.keras.optimizers   import Adam                  # adam
from tensorflow.random             import set_seed              # semilla aleatoria
#from tensorflow.keras.optimizers   import AdamW                 # adamW
#from tensorflow.keras.callbacks    import EarlyStopping         # early stopping
#from tensorflow.keras.callbacks    import ModelCheckpoint       # modelo checkpoint
from tensorflow.keras.constraints  import max_norm              # recorte de grtadiente

"""# A. Entrenamiento ideal"""

# SEMILLA DE GENERADOSRE ALEATORIOS
set_seed(123)
np.random.seed(432)

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)               #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido

# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
modelo = Sequential()
#modelo.add(Dense(1, input_dim=1, kernel_constraint=max_norm(3)))
modelo.add(Dense(20, input_dim = 1))
modelo.add(Dense(1))
#modelo.add(BatchNormalization())
#modelo.add(Dropout(0.5))

# COMPILAR EL MODELO
tasa_aprendizaje = 0.001
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))

# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)
#modelo.fit(X, Y, epochs=1000, verbose=0)

"""# B. Casos

## Síntoma 1: la pérdida aumenta y puede llegar a ser infinita

Causas:
- Tasa de aprendizaje muy grande
- Gradientes que explotan
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)               #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
modelo = Sequential()
modelo.add(Dense(50, input_dim = 1))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.9
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = 0.9))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""Solución: reducir la tasa de aprendizaje"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)               #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
num_neuronas = 50
modelo = Sequential()
modelo.add(Dense(num_neuronas, input_dim = 1))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""Solución; Optimizar la tasa de aprendizaje con Adam"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)               #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
num_neuronas = 50
modelo = Sequential()
modelo.add(Dense(num_neuronas, input_dim = 1))
modelo.add(Dense(1))
# COMPILAR EL MODELO
#tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = Adam())
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""## Síntoma 2: La pérdida tiene valores NaN

Causa: datos faltantes
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)               #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido

# AGREGAR DATOS NaN
X[0] = np.nan
Y[1] = np.nan

# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
num_neuronas = 50
modelo = Sequential()
modelo.add(Dense(num_neuronas, input_dim = 1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""Solución: preprocesar los datos faltantes

---

## Síntoma 3: No hay datos faltantes, pero la pérdida sigue teniendo valores NaN

Causa: los datos no están escalados y en consecuencia los gradientes explotan
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1) * 10000        #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
num_neuronas = 50
modelo = Sequential()
modelo.add(Dense(num_neuronas, input_dim = 1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""Solución: aplicar un escalamiento (estandarización)"""

from sklearn.preprocessing import StandardScaler
# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1) * 10000        #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# ESCALAMIENTO
scaler_x = StandardScaler()
scaler_y = StandardScaler()
X = scaler_x.fit_transform(X)
Y = scaler_y.fit_transform(Y)
# CREAR RED NEURONAL: entrada -> 20 neuronas -> salida
num_neuronas = 50
modelo = Sequential()
modelo.add(Dense(num_neuronas, input_dim = 1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 50)

"""## Síntoma 4: La pérdida se queda estancada

Causas:
- El gradiente se desvance, la tas de aprendizaje es pequeña y el gradiente calcula valores pequeños, y la pérdida no cambia
- La pérdida alcanza un mínimo local, y no logra salir para alcanzar un mínimo bsoluto
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1) * 10          #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(10, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 500)

"""Solución: Aumentar la tasa de aprendizaje

"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1) * 10          #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(10, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.5
modelo.compile(loss='mean_squared_error', optimizer = Adam())
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 500)

"""Solución: Cambiar de optimizador como Adam"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1) * 10          #  X = valor aleatorio entre 0 y 1
Y = 2*X + np.random.rand(100,1)*0.1     # Y = 2*X +ruido
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(10, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = Adam())
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 500)

"""## Síntoma 5: La pérdida nuevamente comienza a crecer y se sale de control

Causa: Los gradientes se hacen muy grandes
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 10*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
for _ in range(50):
  modelo.add(Dense(50, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.8
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: reducir las capas ocultas"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 10*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
for _ in range(50):
  modelo.add(Dense(1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.8
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: Reducir la tasa de aprendizaje"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 10*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
for _ in range(50):
  modelo.add(Dense(50, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: Cambiar de optimizador"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 10*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
for _ in range(50):
  modelo.add(Dense(50, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = Adam())
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""## Síntoma 6: La pérdida oscila y no se estabiliza

Causa: alta tasa de aprendizaje
"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 2*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.9
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: reducir la tasa de aprendizaje"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 2*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: Incluir batch normalization"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 2*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(BatchNormalization())
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)

"""Solución: Aplicar recorte de gradientes (gradient clipping)"""

# GENERANDO DATOS ALEATORIOS
X = np.random.rand(100,1)
Y = 2*X + np.random.rand(100,1)*0.1
# CREAR RED NEURONAL: entrada -> 10 neuronas -> 10 neuronsa -> salida
modelo = Sequential()
modelo.add(Dense(10, input_dim = 1, activation = 'relu'))
modelo.add(Dense(10, activation = 'relu', kernel_constraint=max_norm(1.0)))
modelo.add(Dense(1))
# COMPILAR EL MODELO
tasa_aprendizaje = 0.1
modelo.compile(loss='mean_squared_error', optimizer = SGD(learning_rate = tasa_aprendizaje))
# ENTRENAR EL MODELO
modelo.fit(X, Y, epochs = 100)